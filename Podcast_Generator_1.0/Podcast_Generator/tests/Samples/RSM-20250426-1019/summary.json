[
  "- **Publication:** \"Data augmentation and language model adaptation\" (ICAS SP-88, 1988)\n- **Auteurs:** David Janiszek (Université de Paris), Renato De Mori (McGill University & University of Avignon), F. Bechet\n- **Résumé:**\n  - Méthode d'augmentation des comptages de n-grammes de mots dans une matrice représentant un modèle de langage 2-grammes à l'aide de distances numériques dans un espace réduit obtenu par décomposition en valeurs singulières (SVD).\n  - Réévaluation des grilles de mots dans les applications de dialogue vocal avec un modèle de langage contenant des comptages augmentés entraîne une réduction du taux d'erreur sur le mot (WER) de 6,5 %.\n  - Interpolation supplémentaire des comptages augmentés avec des comptages provenant d'un grand corpus de journaux, mais uniquement pour des historiques sélectionnés, aboutit à une réduction totale du WER de 11,7 %, surpassant l'interpolation des comptages globaux.\n\n- **Introduction:**\n  - Les systèmes de reconnaissance vocale automatique (ASR) génèrent des hypothèses de mots basées sur les modèles linguistiques fournissant des distributions de probabilité pour chaque mot et classe d'histoires.\n  - Les ML formés dans un domaine peuvent mal fonctionner lorsqu'ils sont appliqués à un autre, en particulier dans les systèmes de dialogues avec des probabilités dépendantes du sujet.\n  - Diverses méthodes d'adaptation de ML existent, notamment le regroupement de données, l'interpolation linéaire, le repli, la récupération de documents, l'entropie maximale/minimum discrimination, l'adaptation MAP et la transformation vectorielle.\n  - L'approche proposée conjecture que les mots sémantiquement similaires sont susceptibles d'apparaître dans le même contexte que les mots observés.\n\n**Résumé:**\n\n1. **Définition de la Similarité des Mots:**\n   - Définie à l'aide de la distance numérique entre les vecteurs représentant les mots dans un espace approprié.\n   - L'espace est défini à l'aide de la décomposition en valeurs singulières (SVD) issue de l'approche de récupération d'informations.\n   - Les mots sémantiquement similaires sont ceux représentés par des vecteurs à l'intérieur d'un cône autour du vecteur du mot cible.\n   - La similarité",
  "**Summary:**\n\n- **Publication Details:** \"Data augmentation and language model adaptation\" (ICAS SP-88, 1988)\n- **Authors:** David Janiszek (Université de Paris), Renato De Mori (McGill University & University of Avignon), F. Bechet\n- **Abstract:**\n  - A method to augment word n-gram counts in a matrix representing a 2-gram Language Model (LM) using numerical distances in a reduced space obtained by Singular Value Decomposition (SVD).\n  - Rescoring word lattices in spoken dialogue applications with an LM containing augmented counts leads to a Word Error Rate (WER) reduction of 6.5%.\n  - Further interpolating augmented counts with counts from a large newspaper corpus, but only for selected histories, results in a total WER reduction of 11.7%, outperforming global count interpolation.\n- **Introduction:**\n  - Automatic Speech Recognition (ASR) systems generate word hypotheses based on language models providing probability distributions for each word and history class.\n  - LMs trained in one domain may perform poorly when applied to another, especially in dialoguesystems with topic-dependent probabilities.\n  - Various LM adaptation methods exist, including data pooling, linear interpolation, back-off, document retrieval, maximum entropy/minimum discrimination, MAP adaptation, and vector transformation.\n  - The proposed approach conjectures that semantically similar words are likely to appear in the same context as observed words.",
  "**Summary:**\n\n1. **Word Similarity Definition:**\n   - Defined using numerical distance between vectors representing words in a suitable space.\n   - Space defined using Singular Value Decomposition (SVD) from Information Retrieval approach.\n   - Semantically similar words are those represented by vectors within a cone around the target word's vector.\n   - Similarity expressed as distance between vectors of two words.\n   - Word count augmented by weighted contribution of semantically similar words.\n\n2. **Word Representation in Reduced Space:**\n   - Words represented by vectors using SVD on matrix P (words x histories).\n   - Matrix P approximated as U * S * V^T, where U and V are matrices of eigenvectors, and S is a diagonal matrix of singular values.\n   - Word and history vectors mapped to reduced space using transformation matrix U.\n\n3. **Experimental Setup:**\n   - AGS system performed first-pass recognition for test set and provided word hypothesis trellises.\n   - Experiments aimed to reduce Word Error Rate (WER) by rescoring hypotheses with new Language Models obtained through data augmentation or adaptation, using the same acoustic model scores.\n   - Telephone corpus used: 9842 sentences for training and 1419 word lattices produced by AGS system for testing.",
  "**Summary:**\n\n- **Corpus Split:**\n  - Total corpus (T) arbitrarily split into 1000 lattices.\n  - Test corpus (T) further divided into T+/ and T-/ sets: 280 lattices in T+ (correct transcription matches existing path), 120 in T-.\n\n- **Baseline Performance:**\n  - Word Error Rate (WER) for baseline system:\n    - T: 27.1%\n    - T+: 11.26%\n    - T-: 50.81% (suggests sentences should be rejected)\n\n- **Data Augmentation on Distances between Word Representations:**\n  - Formula to calculate augmented count (aij) using similarity degree (f) of histories hj and hk.\n  - Euclidean distance used initially, then quasi-K-nearest histories approach with K=7 and D=1 showed better results.\n\n- **Quasi-K-Nearest Histories Augmentation Results:**\n  - WER gain for T+ and T- sets:\n    - T+: 6.25%\n    - T-: 6.0%",
  "- **Constraint Satisfaction**: Word row and corresponding column sum used to re-establish constraint satisfaction.\n- **Weighted Count Augmentation**: Linear interpolation of LM probabilities for adaptation (In[1]).\n  - Bigram counts: cg (general corpus), cd (domain adaptation corpus). Adaptation: c = λcg + (1-λ)cd.\n  - Global interpolation results (Table 3): Small gains but better than no adaptation.\n  - Specific histories' count augmentation improves results, complementing word-level augmentation.\n  - Constraints-based histories (e.g., pronouns in French) should be selected for augmentation with suitable weights (/#0B /#28 wj /#29).\n- **Selected Histories Adaptation**: New count matrix for each history j (d & g corpora), LM calculation, and decoding. Gain in WER on development corpus determines inclusion in list Lh.\n  - New adaptation between Md and Mg for histories in Lh with calculated weights (/#0B /#28 wj /#29).\n- **Results**: Significant improvements on T+ (Table 5), but degradation on T and T/. Prioritizing reliable hypotheses over rejected sentences.",
  "**Summary:**\n\n- **Method:** A new data augmentation method is proposed, based on distances between word representations in reduced space.\n- **Results (Table 4):** The method improves language models (LMs), reducing Word Error Rate (WER).\n- **Limitation:** WER reduction may not always be significant due to dependence on acoustic score quality.\n- **Benefit:** Allows training accurate LMs for different dialogue situations, aiding rescorings and decision algorithms.\n- **Future Work:** Exploring decision-making with hypotheses from competing models applied on a trellis of hypotheses is promising.\n- **References:** Nine key works in language model adaptation and speech processing are cited."
]