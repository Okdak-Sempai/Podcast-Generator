See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/3908401
Data augmentation and language model adaptation
Conf erence Paper    in  Acoustics, Speech, and Signal Pr ocessing, 1988. ICAS SP-88., 1988 Int ernational Conf erence on  · Februar y 2001
DOI: 10.1109/ICAS SP.2001.940890  · Sour ce: IEEE Xplor e
CITATIONS
9READS
117
3 author s, including:
David Janisz ek
Univ ersité de P aris
15 PUBLICA TIONS    49 CITATIONS    
SEE PROFILE
Renat o De Mori
McGill Univ ersity and Univ ersity of A vignon
343 PUBLICA TIONS    6,847  CITATIONS    
SEE PROFILE
All c ontent f ollo wing this p age was uplo aded b y Renat o De Mori  on 24 Dec ember 2013.
The user has r equest ed enhanc ement of the do wnlo aded file.DATAAUGMENTATIONANDLANGUAGE MODEL ADAPTATION
D. Janiszek, R. De Mori, F. Bechet
LIA-University ofAvignon
84911AvignonCedex9-France
ABSTRACT
A methodis presented foraugmentingword n-gram counts
inamatrixwhichrepresentsa2-gramLanguageModel(LM).This method is based on numerical distances in a reducedspace obtained by Singular Value Decomposition (SVD).
Rescoringwordlatticesinaspokendialogueapplicationus-
inganLMcontainingaugmentedcountshasleadtoaWordError Rate (WER) reduction of 6.5%. By further interpo-lating augmented counts with the counts extracted from averylargenewspaper corpus,butonlyforselectedhistories,a total WER reduction of 11.7% was obtained. We show
that this approach gives better results than a global count
interpolation forallhistoriesoftheLM.
1. INTRODUCTION
Most of the existing automatic speech recognition (ASR)
systems generate word hypotheses by computing the prob-ability of a sequence of wordsW
N/1as a product of condi-
tionalprobabilities ofwords wiand their histories hi.L a n -
guagemodels( LM)provideprobabilitydistributions P /#28 wi
j hi
/#29
for each word wiof the vocabulary and for each history
class hi. Probabilities P /#28 wi
j hi
/#29areestimatedusingatrain-
ing corpus. In practice, their value and the accuracy of the
estimation depend on the corpus.
U s u a l l ya nL Mt r a i n e di nad o m a i n D/1exhibits poorer
performance when applied to a domain D/2with respect to
an LM trained in the new domain D/2.T h i si sa l s ot r u ei n
dialoguesystemsinwhichtheprobabilityofwordsandhis-tories often depend on dialogue situations, especially when
the machine asks questions about speciﬁc topics. Different
solutions exist to overcome the problem of LM variations.Iflargecorporaarenotavailable, thenLMs canbeadapted.VariousmethodsforLMadaptationhavebeenproposedandreviews of them can be found in [1]. Most of the proposed
methods perform dynamic adaptation, consisting in contin-
uouslyupdatingintimetheLMprobabilitydistributions.
There are many methods for adapting to a new domain
whichcanbegroupedintothefollowingclasses:
This research is supported by France Telecom’s R&D under the con-
tract971B427
/#0Ftraining an LM in the new domain if sufﬁcient data
areavailable,/#0Fpooling data of many domains with data of the new
domain,/#0Flinearinterpolationofageneralandadomain-speciﬁcmodel[2],/#0Fback-off of domain speciﬁc probabilities with thoseofageneralmodel[3],/#0Fretrieval of documents pertinent to the new domain
and training a new LM on-line with thosedata [4],/#0Fmaximum entropy, minimum discrimination adapta-
tion[5],/#0FMaximumAPosterioriProbability(MAP)adaptation[1],/#0Fadaptation by linear transformation of vectors of bi-gram counts in areducedspace[6].
Another possibility could be that of performing data
augmentation by inferring counts for the training set based
on the available adaptation data, in such a way that LM
probabilities are estimated from counts obtained only from
the adaptation data augmented with counts generated by asuitablesmoothing/generalization criterion. The approach
proposed in this paper starts with the conjecture that if aword has been observed in a given context, then semanti-
cally similar words are likely to appear in the same context
evenifthiseventwasnotobservedintheadaptationcorpus.Semantic similarity between words can be deﬁned using anumericaldistancebetweenvectorsrepresentingwordsinasuitable space. Following an approach of Information Re-
trieval[7],suchaspacecanbedeﬁnedusing SingularValue
Decomposition (SVD).
Given a vector representing a wordW, it is possible to
deﬁne a cone around it and consider all words representedby vectors inside this cone as semantically similar toW.
The similarity between a word W
/0i nt h ec o n ea n d Wcan
beexpressedbyadistancebetweenthevectors representing
thetwowords. Thecountof Wina givencontextcan thusbe augmented by a contribution from the count of W
/0in
the same context weighted by a decreasing function of the
distance between the vectors representing the two words.Section 4 details thisapproach.
Itisimportanttopointoutthatallthemethodspresented
inthispaperdonotleadtoanyincreaseintheLMcomplex-ity.
2. REPRESENTATION OF WORDS INREDUCED
SPACE
Representation of words by vectors in a reduced space can
be obtained [8], [7] by SVD based on which any matrixP
ofdimensions I/; Jand rank rcan be expressed as:P /= L
/#14/#01 /0/0 /0
/#15M
T(1)
where Land Mare orthogonalmatrices, their columns
are orthonormal, M
Tindicates the transpose matrix of M,
and /#01is the diagonal matrix of the positive eigenvalues ofPP
T. The order of /#01is f r /;r g.T h e c o l u m n s o f Lare
an orthonormal set of basis vectors spanning the range. Byconsidering only theqprominent eigenvalues, a diagonal
matrix Scanbebuiltwiththeﬁrst qsingularvaluesof PP
T
in decreasing order such that:P
/#18/=
US V
T(2)
wherethe Uhas qcolumnsconsistingoftheﬁrst qeigen-
vectors of PP
Twhile Vis made with the ﬁrst qeigenvec-
torsof P
TP.
Matrices U, Sand Varecomputedwithaniterativepro-
cedureasproposedin[8]foravalueof qchoseninasucha
waythat s/0
/=sq(with sqthe q-thsingularvalue)approximate/1/0
/3, which has been found to be a reasonable compromise
between accuracy and computational complexity.
Matrix Pis, in our case, a matrix having rows corre-
sponding to words and columns to histories. It can be used
to ﬁnd the transformation for mapping word and historyvectors into thereducedspace.
LetP /= f pij
gbe a I /#02 Jmatrix where the generic
element f pij
grepresents the count of observations of wordWiinthecontextofhistory hj.T h e i-throwofmatrix Pisa
vector whose Jelementsarethecountsof wiinallpossible
histories.
Vector PicanberepresentedinreducedspacebyvectorRiobtainedasfollows:Ri
/= U
TPi (3)Because the number of columns of Uand Vis much
smallerthanthenumberofcolumnsin P,itisexpected that
not many elements of Riareequaltozero.
If Phas been trained on a very large corpus containing
a good mix of topics, one may assume that the estimatedeigenvalues which are the non-zero elements of matrixS,
aretypicalofalanguageand do not varyfrom oneapplica-
tion to another. This conjecture has been validated experi-mentallyasitwillbeshownlater on usingacorpusmadeof/4 /0million words from articles of the French newspaper Le
Monde.
3. EXPERIMENTALSETUP
Asystem,called AGS,describedin[9],anddeployedonthe
telephonenetwork, performedaﬁststeprecognitionforthetest set, and made available, for each test sentence, a trellisof wordhypothesesaswellas thebesthypothesisproducedby the system. Let such a baseline system be indicated asB.L e t TB
/#28 k /#29bethetrellis providedbythebasesystem for
the k-thsentence. Thepurposeoftheexperimentsis thatof
assessing ifand howmuch theWord ErrorRate(WER)canbereducedbyrescoringthewordhypothesesinTB
/#28 k /#29with
new LMs obtained after data augmentation or adaptation,
by using the same scores provided by the acoustic models
when TB
/#28 k /#29was generated.
Experiments were carried out using a telephone corpus
of sentences from person-machine dialogues collected byFrance-Telecom R&D in fairly severe conditions all overFrance.
The corpus contains a training set of 9842 sentences.
Anothersetof1419wordlatticesproducedbytheAGSsys-temwas arbitrarily split in a 1000 latticesdevelopment cor-pusDanda419latticestestcorpus T. Furthermore,thetest
corpus Thas been split in two sets: T
/+which contains the
280latticesforwhichthecorrecttranscriptionisasequence
of words that corresponds to an existing path in the latticeandT
/,whichcontainsthe120remaininglattices. Wewill
present the results according to these two corpus becauseour aim is to precisely evaluate the gain of our rescoringmethod when all the information is available in the lattice
produced by the ﬁrst decoding process. Table 1 shows the
baselineperformancesofBonthethreecorpora T, T
/+andT
/,.
corpus T T
/+T
/,
WER27.1211.2650.81
Table 1. Resultsofthebaselinesystem B
Thepoorresultsobtainedon T
/,leadustoconsiderthat,
with more than /5/0 /#25WER, these sentences should be re-
jectedbythedialoguesystem. Forthisreasonwewillfocusontheresults obtainedon T
/+.
4. DATA AUGMENTATIONBASED ONDISTANCES
BETWEEN WORDREPRESENTATION
Let cijbe the count of times the word wihas been really
observed in theadaptationdatainthecontextof history hj.
Let aijbethecountforthesamewordandhistory,butafter
data augmentation. Let /,
/#0Bj
/#28 /#12 /#29be the set of vectors repre-
senting the histories having distance lower than a threshold/#12from thevectorsrepresenting hjin the reduced space S/#0B.
Let d
/#0Bjkbe the distance between vectors representing histo-
ries hjand hkin reduced space S/#0B. The augmented countaijofthe sequence hj
wiisobtainedassuming ahistory hk
similarto hjcontributes tothecounts of thesequence hj
wi
inawaythatdependsonadegreeofsimilarity betweenthe
twohistories hjand hk:ai j
/= cij
/+
Xhk
cik
/#02 f /#28 d
/#0Bik
/#29 (4)
with: hk
/2 /,
/#0Bj
/#28 /#12 /#29
Thedegreeofsimilarityisexpressedbyafunction f /#28 d
/#0Bik
/#29
ofthedistancebetweentherepresentationsofthetwohisto-
ries. Thefunction f /#28 d
/#0Bik
/#29shouldbeequalto /1when d
/#0Bik
/=/0
and should decrease with d
/#0Bik. A reasonable assumption is
thefollowing:f /#28 d
/#0Bik
/#29/= e
/,
d
/#0BikD (5)
where Dis a parameter that can be used for tuning the
system. TheEuclidiandistancebetweeneachpairofhistoryvectorswascomputedinreducedspace. Theanglebetweeneach pair of historyvectorwas alsoconsidered becauseit isused in InformationRetrievalbutitwasabandoned becauseitproducedpoorresults.
A ﬁrst experiment was performed using a function de-
ﬁned by equation 5 withD /=/1with contribution from
all histories. Results did not show a strong improvementwith data augmentation, but suggested that history depen-dentthresholdsshouldbeused. Anumberofdifferentideaswas considered. Itwas found that augmenting each bigramcount with contributions only from the K nearest histories
was effective. It may happen, that, for a givenK,t h e r ea r e
manyhistorieswithveryclosedistancesw.r.t. the K-thone.
Consequently, contributions from all these histories werealso considered after having empirically selected a thresh-old for considering distances to be practically equivalent.
Letus callthis approach quasi-Knearesthistories .
The parametersKand Dhave been calculated on the
development corpus, and the following values have beencorpus T T
/+T
/,
WER /2/6 /: /9/5 /1/0 /: /5/3 /5 /1 /: /1 /4
gain /+/0 /: /6/#25 /+/6 /: /5 /#25 /, /0 /: /6/#25
Table 2. Resultsofthequasi-Knearesthistories augmenta-
tion: A/1
chosen: K /=/7 /7, D /=/1. The results are presented in
table 2.
With these types of count adaptation, some of the con-
straints thatshould hold between row and columnmarginalcountsmaynolonger hold. Evenifinpractice, thediscrep-ancy is small between the sum of counts for a word row
and the sum of counts in the column corresponding to the
same word considered as history, it is possible to reestab-lish constraint satisfaction. Detailsareomitted for the sakeof brevity.
5. ADAPTATIONWITH WEIGHEDCOUNT
AUGMENTATIONONSELECTEDHISTORIES
In[1], itis shownthatMAP adaptation of LMprobabilities
can be performed by a linear interpolation of the a-prioriprobabilitiesprovidedbythegeneralLMandtheprobabili-tiesobtainedwiththeadaptationcorpus. Thesameideacan
be applied to bigram counts. Letcg
/#28 wi
/;wj
/#29 cd
/#28 wi
/;wj
/#29be
the bigram counts, respectively, in the general corpus andin the domain adaptation corpus. An adaptation can be per-formedbyinterpolationofthegeneralmodelcountsandthecountsoftheadaptationcorpus.
Table3showstheresultsobtainedbyapplyingthismethod
toallthehistoriesofanLMtrainedwiththecorpusfrom Le
Monde(g) using the training corpus of domain speciﬁc ( d)
dataforadaptation.
corpus T T
/+T
/,
WER /2/6 /: /9/5 /1/1 /: /0/4 /5 /0 /: /3 /8
gain /+/0 /: /6/#25 /+/1 /: /9 /#25 /+/0 /: /8/#25
Table 3. Results oftheglobalinterpolation: A/0
Eveniftheresultsarebetter,thegainobservedisrather
small. Nevertheless, larger improvements have been ob-served by using counts from somespeciﬁc histories ofgto
augmentcounts forthesamehistories of d. In fact, adapted
andaugmented countscanbeexpressedasfollows:ca
/#28 wi
/;wj
/#29/= /#0B /#28 wj
/#29 /#02 cg
/#28 wi
/;wj
/#29/+ cd
/#28 wi
/;wj
/#29(6)
The same type of data augmentation is applied to the
counts of all words having a given history, while the typeof augmentation proposed in the previous section was ap-
pliedtoeachwordcount,usingcontributionsfromdifferent
histories. The two types of augmentation are thus comple-mentary.
Selecting the histories to be used for augmentation is
crucial. There are histories represented, for example, bypronouns, for which the words which may follow have to
satisfy certain constraints, like, in French gender and num-
ber. Itispossiblethatsomeofthewordssatisfyingthecon-straints are not observed in thedcorpus, but are present in
the gcorpus. Thesewordsshould haveaprobabilityhigher
thantheone obtainedwith back-offmethods, butsomehowlowerthan what they haveing, because the words may not
be semantically as relevant in das they are in g. Histories
having a large chance of inducing constraints on the fol-lowing words have to be selected and the weight/#0B /#28 wj
/#29of
countaugmentationinducedbythesehistoriesmustbesuit-ablychosen andhas tobehistorydependent.
To choose these histories, we calculated a new count
matrix for each historyj. In this matrix, we combine the
counts of the history jfrom the corpora dand gby means
of equation 6. The parameter /#0B /#28 wj
/#29was set constant and
equal to the ratio of words in the domain and the generalcorpus.
Then, an LM was calculated on each matrix and a de-
codingprocesswasperformed. Everyhistoryjwhichleads
to a gain in WER on the development corpus was stored in
al i s t Lh. At the end of this process, we performed a new
adaptation between the counts of Mdand Mgonly for the
histories stored in Lh. In this process, a value /#0B /#28 wj
/#29was
calculatedforeachhistory jonthedevelopmentcorpus. By
c om bini ngt hisad apt a tio nwit htheonep rese nte dinse ct io n
4, weobtained theresults A/2wh ic haresh o wnint abl e5.
Theresultsaresigniﬁcantlybetteron T
/+,evenifadegra-
dationisobservedon Tand T
/+. Webelievethatimproving
reliablehypothesesismoreimportantthatdegradingresultsonsentenceswhichwillberejectedanywaybythedialoguemodule.
corpus T T
/+T
/,
WER /2/7 /: /5/6 /9 /: /9 /4 /5/3 /: /5/5
gain /, /1 /: /6/#25 /+/1/1 /: /7/#25 /, /5 /: /3/#25
Table 4. Results oftheaugmentationmethod A/2
6. CONCLUSION
A newmethodfordataaugmentation has been proposed. It
isbased on distancesbetween word representationsin are-
duced space. The method leads to better LMs with which
lowerWERareobtained. TheimprovementintheLMmaynot always lead to a big reduction in WER because its im-
pact depends on the quality of the acoustic scores. Never-
theless, data augmentation and count reuse makes it possi-ble to train different LMs for different dialogue situationswith greater accuracy. These LMs can be used in a rescor-ing phase and new decision algorithms can be conceived
based on the results obtained with different LMs. Decision
making in presence of hypotheses generated by different,competing models applied just on a trellis of hypothesesemerges now asa promising research direction.
7. REFERENCES
[1] FedericoM.andDeMoriR., LanguageModelAdapta-
tion, K.Pontinged.Springer-Verlag,Berlin,NewYork,
1999.
[2] Seymore K. and Rosenfeld R., “Using story topics for
language model adaptation,” in Proc. Eurospeech 97,
Rhodes,Greece , 1997.
[3] Besling S. and Meier H.G., “Language model speaker
adaptation,” in Proc. Euospeeech95 pp. 1755-1758,
Madrid, Spain ,1995.
[4] Iyer R. and Ostendorf M., “Modeling long distance
dependence in language: topic mixtures vs. dynamic
cache models,” in IEEE Transactions on Speech and
Audio processingSAP-7(1):30-39 , 1999.
[5] ChenS.F.,SeymoreK.,andRosenfeldR., “Topicadap-
tation for language modeling using unnormalized ex-ponential models,” in IEEE Intl. Conf. on Acoustics,
SpeechandSignalProcessing, SeattleWA , 1998.
[6] Janiszek D., de Mori R., Bechet F., Matrouf D., and
MokbelC., “Newlanguagemodeladaptationalgorithm
based on the deﬁnition of cardinal distance,” in ESCA
WorkshoponInteractiveDialoguein Multi-ModalSys-tems, KlosterIrsee, Germany ,1999.
[7] BellagardaJ, “Multi-spanstatisticallanguagemodeling
for large vocabulary speech recognition,” IEEE Trans-
actionsonSpeechandAudioprocessingSAP-6(5):456-
467, 1998.
[8] BerryM.W., “Large-scalesparsesingularvaluecompu-
tations,” in Int. J. Supercomp. Appl. Vol6, No 1, pp13-
49, 1992.
[9] SadekD.,FerrieuxA., CozannetA.,BretierP.,Panaget
F., and Simoni J., “Effective human-computer cooper-ative spoken dialogue: the ags demonstrator,” in IC-
SLP’96, USA , 1996.
View publication stats